\input{suhw.tex}
\usepackage{graphicx,amssymb,amsmath,enumerate}
\usepackage{courier}
\usepackage{color}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{stmaryrd}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\lstset{language=Python,
	frame=lines,
   basicstyle=\ttfamily\fontsize{8}{12}\selectfont,
   keywordstyle=\color{blue},
   commentstyle=\color{red},
   stringstyle=\color{dkgreen},
   numbers=left,
   numberstyle=\tiny\color{gray},
   stepnumber=1,
   numbersep=10pt,
   backgroundcolor=\color{white},
   tabsize=2,
   showspaces=false,
   showstringspaces=false,
   lineskip=-3.5pt }
\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in

\begin{document}

\normaldoc{CS276: Information Retrieval and Web Search}{Spring 2013}{Programming Assignment 4}{Botao Hu (botaohu), Jiayuan Ma (jiayuanm)}{\today}

\pagestyle{myheadings}  % Leave this command alone

\section{Task 1}
For this task, we use sublinear scaling and inverse document frequency when constructing query vectors.
For document vectors, we use term frequency (without any scaling) and length normalization.
For length normalization, we divide all fields by their own field lengths.
We use tf-idf scores for each field (title, url, body, header, anchor) as features.

1. Report the NDCG performance achieved on the development test data. How does this score compared to the performance obtained using the weights manually set in PA3?

The NDCG performance on the development test data is $0.8632$, compared to the NDCG score $0.8662$ achieved in PA3.

2. Do the values of $w^*$ make sense (e.g., should a field be weighed higher than another)? Report your finding.

The weights learned in this task and manually tuned in PA3 are compared in Table \ref{tab:wt}.
As we can see from Table \ref{tab:wt}, there are not any significant similarities between them.
According to the weights produced by linear regression, body field is the most important feature. On the other hand,
anchor field is the least important feature, which is questionable. Since linear regression is not robust with outliers,
the learned weights are not reliable enough to make any importance judgements.
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
  \hline
  Weight $w^*$ & $w_\textrm{title}$ & $w_\textrm{url}$ & $w_\textrm{header}$ & $w_\textrm{body}$ & $w_\textrm{anchor}$ \\
  \hline
  Pointwise linear regression weights & $0.2$ & $0.5$ & $0.1$ & $1.5$ & $0.0$ \\
  \hline
  Weights manually tuned in PA3 & $1.0$ & $0.1$ & $0.5$ & $0.3$ & $2.0$ \\
  \hline
\end{tabular}
\caption{Weights obtained by two different approaches}\label{tab:wt}
\end{center}
\end{table}


\section{Task 2}
In this task we use the same feature as in task 1, but we operate in the pairwise space.
For any pairs of query-document vector features $\mathbf{x}_i, \mathbf{x}_j$, we compute the difference between them $\mathbf{x}_i - \mathbf{x}_j$ and label them using the relevance score between them. 

1. Report the NDCG performance achieved on the development test data.

The NDCG score on the development test data is $0.8688$.


2. Compare your ranking outputs given by the linear regression and the ranking SVM.
Find 2-3 queries in which results of one method are markedly different from those
of the other and use your relevance score file to judge which one is better.
Then, for each of these queries, pick a URL and examine the train data file (or the
page content) with reference to the weight vectors learned to find out why that
URL is ranked higher/lower in one method and lower/higher in the other. Report
your finding.

\textbf{TODO}

\section{Task 3}
In this task, we try out the following features
\begin{itemize}
  \item[] tf-idf scores of five different fields (the same setup as in task 1)
  \item[] binary feature indicating if a URL ends in ``.pdf''
  \item[] BM25F value derived with manually tuned weights in PA3 (without the PageRank term)
  \item[] Smallest window features of five different fields
  \item[] PageRank score of each document
\end{itemize}
We build features, calculate the differences between features vectors, and feed these pairwise features into SVM.

1. From the list of features above, find out which combinations of features help boost
performance. Report the NDCG scores achieved on the development test data
for these combinations.

The best NDCG score achieved on the development test data is $0.8871$.
We use all the above features except the smallest window feature for anchor fields.
Since anchor fields are short, smallest windows in bodies are usually quite large and uninformative. Therefore, it is reasonable to exclude this feature from our feature set.

By dropping one feature at a time, we experiments with different combinations of features to see the contributions of different features.
\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
  \hline
  Leaving out feature & NDCG score \\
  Binary ''.pdf'' feature & $0.8868$ \\
  BM25F score & $0.8829$ \\
  Title window & $0.8849$ \\
  URL window & $0.8841$ \\
  Body window & $0.8829$ \\
  Header window & $0.8858$ \\
  Anchor window & $\mathbf{0.8871}$ \\
  Page Rank & $0.8822$ \\
  \hline
\end{tabular}
\caption{NDCG scores with different feature combinations}\label{tab:perform}
\end{center}
\end{table}


2. Examine your ranking output, list 1-2 types of errors that your system tends
to make, and suggest features to fix them. Do they help fix the problems you
observed? What about the NDCG scores achieved on the development test data?
Report your finding. You could also propose new features that help improve performance.
Report the best NDCG score achieved on the development test data.

\textbf{TODO}


\section{Extra Credit (PRank)}

1. Report performance evaluated on the development set

The NDCG score achieved by PRank algorithm on the development test data is $0.8468$, compared to $0.8632$ in task 1.

2. Briefly discuss why one method is better than the other.

\textbf{TODO}
\end{document}

