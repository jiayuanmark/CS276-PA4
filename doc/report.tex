\input{suhw.tex}
\usepackage{graphicx,amssymb,amsmath,enumerate}
\usepackage{courier}
\usepackage{color}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{stmaryrd}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\lstset{language=Python,
	frame=lines,
   basicstyle=\ttfamily\fontsize{8}{12}\selectfont,
   keywordstyle=\color{blue},
   commentstyle=\color{red},
   stringstyle=\color{dkgreen},
   numbers=left,
   numberstyle=\tiny\color{gray},
   stepnumber=1,
   numbersep=10pt,
   backgroundcolor=\color{white},
   tabsize=2,
   showspaces=false,
   showstringspaces=false,
   lineskip=-3.5pt }
\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in

\begin{document}

\normaldoc{CS276: Information Retrieval and Web Search}{Spring 2013}{Programming Assignment 4}{Botao Hu (botaohu), Jiayuan Ma (jiayuanm)}{\today}

\pagestyle{myheadings}  % Leave this command alone

\section*{Introduction}
In this assignment, we adopted linear regression and support vector machine and tried different features, to predict relevance and rank query results.

Table \ref{tab:all} provides an overall NCDG results we have obtained for different tasks on the development test data.
In the rest of this document, we will describe our methodology and findings.
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|}
  \hline
  Task & Method & Test NDCG \\\hline
  1 &  Linear Regression (Pointwise) & 0.8563 \\\hline
  2 & SVM (Pairwise)  & 0.8623 \\\hline
  3 & SVM with rich features (Pairwise)  & 0.8817 \\ \hline
  Extra & PRank & 0.8439   \\\hline
\end{tabular}
\caption{Summary of results}\label{tab:all}
\end{center}
\end{table}

\section*{Task 1}
In this task, we train a linear regression model using a pointwise method.
As we did in PA3, we represent each query-page pair as a vector of five features.

\begin{itemize}
\item We use sublinear scaling and inverse document frequency when constructing query vectors.
\item For document vectors, we use term frequency (without any scaling) and length normalization.
\item For length normalization, we divide all fields by their own field lengths.
\item We use tf-idf scores for each field (title, url, body, header, anchor) as features.
\end{itemize}

The weights learned in this task and manually tuned in PA3 are compared in Table \ref{tab:wt}.
The NDCG performance on the development test data is $0.8632$, compared to the NDCG score $0.8662$ achieved in PA3.
As we can see from Table \ref{tab:wt}, there are not any significant similarities between them.
According to the weights produced by linear regression, body field is the most important feature. On the other hand,
anchor field is the least important feature, which is questionable. Since linear regression is not robust with outliers,
the learned weights are not reliable enough to make any importance judgements.

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Method & $w_\textrm{title}$ & $w_\textrm{url}$ & $w_\textrm{header}$ & $w_\textrm{body}$ & $w_\textrm{anchor}$ & NDCG \\
  \hline
  Pointwise linear regression weights & $0.20$ & $0.59$ & $0.12$ & $1.45$ & $0.00$ & $0.8632$ \\
  \hline
  Weights manually tuned in PA3 & $1.0$ & $0.1$ & $0.5$ & $0.3$ & $2.0$ & 0.8662 \\
  \hline
\end{tabular}
\caption{Weights obtained by two different approaches}\label{tab:wt}
\end{center}
\end{table}

\section*{Task 2}
In this task, we train an SVM using a pairwise method.
We use the same feature as in task 1, but we operate in the pairwise space.
For any pairs of query-document vector features $\mathbf{x}_i, \mathbf{x}_j$, we compute the difference between them $\mathbf{x}_i - \mathbf{x}_j$ and label them using the relevance score between them.

Because SVMs are not robust to high diversity in values of feature,
we do standardization on the features.

The NDCG score on the development test data is $0.8688$.


Compare your ranking outputs given by the linear regression and the ranking SVM.
Find 2-3 queries in which results of one method are markedly different from those
of the other and use your relevance score file to judge which one is better.
Then, for each of these queries, pick a URL and examine the train data file (or the
page content) with reference to the weight vectors learned to find out why that
URL is ranked higher/lower in one method and lower/higher in the other. Report
your finding.

The top result for two queries returned by Linear Regression and SVM respectively are shown in Table \ref{tab:rel}. As we see, SVM performs better than Linear regression in these two queries.

Linear Regression tends to prefer documents with one or more of the
query terms in the URL, whereas SVM identifies the information need better

The weights in Table 3 suggest why this might be occurring. SVM weights the anchor much more
heavily than any other field, thereby emphasizing other pages pointing at this page with a certain
anchor text. This serves as a proxy for identifying information need. In fact, assigning a higher weight
to the anchor text can be thought of as a substitute for PageRank. In contrast, Linear Regression
places a high weight on the title of the page, which may not always be the best guide to the user’s
information need.

On studying the query results in depth, we notice a few things which suggest potential extra features
for the next task:
For similarly relevant URLs, shorter URLs are usually better (http://cs276.stanford.edu is
better than http://www.stanford.edu/classes/cs276)
• Static URLs are usually more relevant than dynamic URLs. Dynamic URLs tend to be longer
and contain characters such as ’?’.
• The earlier a query term occurs as part of a URL, the more relevant it’s likely to be (same
example as above).
In general, these observations reflect that given two documents which appear equally relevant, we
notice that the user is looking for the more general document. This is perhaps because the general
document is more likely to contain relevant links to other, more specific information. In the next
section, we use these observations to create useful features.



\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
 method & query & top url returned by the method & relevance \\  \hline
linear regression & ee master requirements &  http://ee.stanford.edu/  & 1.0 \\\hline
SVM & ee master requirements  & http://ee.stanford.edu/admissions & 1.66 \\\hline
linear regression & stanford cs track &  http://csmajor.stanford.edu/Considering.shtml &
1.66 \\\hline
SVM & stanford cs track &  http://csmajor.stanford.edu/Requirements.shtml & 2.0 \\\hline
\end{tabular}
\caption{Results of linear regression are markedly different from SVM}\label{tab:rel}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Method & $w_\textrm{title}$ & $w_\textrm{url}$ & $w_\textrm{header}$ & $w_\textrm{body}$ & $w_\textrm{anchor}$ & NDCG \\
  \hline
  Pointwise linear regression weights & $0.20$ & $0.59$ & $0.12$ & $1.45$ & $0.00$ & $0.8632$ \\
  \hline
  Pairwise SVM weightes & 0.12 & 0.35 &  0.14 &  0.06 &  0.03 &  0.8688 \\
  \hline
\end{tabular}
\caption{Weights obtained by two different approaches}\label{tab:wtt}
\end{center}
\end{table}

\section*{Task 3}
In this task, we try out the following features
\begin{itemize}
  \item[] tf-idf scores of five different fields (the same setup as in task 1)
  \item[] binary feature indicating if a URL ends in ``.pdf''
  \item[] BM25F value derived with manually tuned weights in PA3 (without the PageRank term)
  \item[] Smallest window features of five different fields
  \item[] PageRank score of each document
\end{itemize}
We build features, calculate the differences between features vectors, and feed these pairwise features into SVM.

1. From the list of features above, find out which combinations of features help boost
performance. Report the NDCG scores achieved on the development test data
for these combinations.

After experimenting with the above features, the best NDCG score achieved on the development test data is $0.8871$.
We use all the above features except the smallest window feature for anchor fields.
Since anchor fields are short, smallest windows in anchor fields are usually missing and uninformative.
Therefore, it is reasonable to exclude this feature from our feature set.

By dropping one feature at a time, we experiments with different combinations of features to see the contributions of different features. As observed from Table \ref{tab:perform}, we see BM25F score, Page Rank and smallest windows in body fields are huge boosters for performance.
\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
  \hline
  Leaving out feature & NDCG score \\
  \hline
  Binary ''.pdf'' feature & $0.8868$ \\
  BM25F score & $0.8829$ \\
  Title window & $0.8849$ \\
  URL window & $0.8841$ \\
  Body window & $0.8829$ \\
  Header window & $0.8858$ \\
  Anchor window & $\mathbf{0.8871}$ \\
  Page Rank & $0.8822$ \\
  None & $0.8854$ \\
  \hline
\end{tabular}
\caption{NDCG scores with different feature combinations}\label{tab:perform}
\end{center}
\end{table}


2. Examine your ranking output, list 1-2 types of errors that your system tends
to make, and suggest features to fix them. Do they help fix the problems you
observed? What about the NDCG scores achieved on the development test data?
Report your finding. You could also propose new features that help improve performance.
Report the best NDCG score achieved on the development test data.

\textbf{TODO}


\section*{Extra Credit (PRank)}

We use PRank to rank the document relevance. The relevance in training data is derived by averaging editor's opinions, and can be a fraction, and not an integer. 
We approximate the relevance to the nearest integer by rounding before we need the relevence into PRank.  

The NDCG score achieved by PRank algorithm on the development test data is $0.8468$, compared to $0.8632$ in task 1.

PRank has not only 5 paramemters for weights, and it also has 4 threshold parameters $b$ while Linear regression has only 5 parameters. 
Due to a lack of data, even though PRank has more expressibility models, it is also easier to be underfitted. That's why It has lower performance than the linear regression. 

\end{document}

